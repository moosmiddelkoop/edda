---
title: "Assignment 1"
author: "Gijs Smeets, Daan Wijnhorst, Moos Middelkoop, group 35"
date: "February 2023"
output:
  html_document:
    df_print: paged
fontsize: 11pt
highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glue)
```

## Exercise 1. Birthweights

LOADING IN DATA
```{r}
data = read.table('../data/birthweight.txt',header=TRUE)
x = data$birthweight
```


#### A
##### Check normality of the data. Assuming normality (irrespective of your conclusion about normality), construct a bounded 96%-CI for mu. Evaluate the sample size needed to provide that the length of the 96%-CI is at most 100. Compute a bootstrap 96%-CI for mu and compare it to the above CI.

To check for normality we use a Shapiro-Wilk test. The null-hypothesis for this test is that the data is normally distributed. While it is not possible to infer results from a failed rejection of the null hypothesis , the result of this test can tell us if there is no significant non-normality present.

```{r}
shapiro.test(x)
```

Additionally, we create a qqplot to plot the quantiles of our data against those of a normal distribution, to get a visual impression of the normality of the data.

```{r,echo=FALSE,figheight=1.5}
qqnorm(x); qqline(x)
```

The linearity of this plot and a `p=0.8895` gives us a strong impression that our data is normally distributed. Now we can go on to construct the 95% confidence interval (CI) for mu, assuming normality.

```{r}
mean = mean(x)
sd = sd(x)
n = length(x)

ci = c((mean - qnorm(0.98)*(sd/sqrt(n))), (mean + qnorm(0.98)*(sd/sqrt(n))))
```

This results in a confidence interval of [`r round(ci,3)`]

The margin of error is 50. Following theory, we have to show that `qnorm(0.98)*(sd/sqrt(n)) =< 50` to compute the minimal sample size needed to provide that the length of the 96%-CI is at most 100.
```{r}
min_n = ((qnorm(0.98)^2)*(sd^2))/(50^2)
```
This results in `min_n` being `r round(min_n,3)`.


##### bootstrap 96%-CI 
```{r}
B = 1000
Tstar = numeric(B);
for (i in 1:B){
  Xstar = sample(x,replace=TRUE)
  Tstar[i] = mean(Xstar)
}
Tstar20 = quantile(Tstar,0.020)[[1]]
Tstar980 = quantile(Tstar,0.975)[[1]]
bootstrapci = c(2*mean-Tstar980,2*mean-Tstar20)
```

Next we compare the length of the bootstrap 96% CI to the bounded 96% CI
```{r}
lenci = ci[2]-ci[1]
lenbsci = bootstrapci[2]-bootstrapci[1]
glue('CI length: {lenci},    Bootstrap CI length: {lenbsci}');
```
This shows that the length of the bootstrap CI is smaller, which indicates that it is more accurate.

 
#### B
##### An expert claims that the mean birthweight is bigger than 2800 gram. Verify this claim by using a relevant t-test

In order to verify this claim, we perform a t-test
```{r}
t.test(x, mu=2800)
```
The p-value 0.02713 < 0.05, and the mean of the sample is 2913, we can reject the null-hypothesis and conclude that the mean birthweight is statistically significantly bigger than 2800 grams.

This result is already statistically significant. One could also argue that a one-sided t-test can be justified. because it's an experts claim that babies will weigh more than 2800 grams.
This would make `H0 = mu > 2800`. 

```{r}
t.test(x, mu=2800, alt='g')
```

A one-sided test is less strict than a two-tailed test and as expected, the p value is twice as low for the one-sided test as for the two-tailed test. 

##### Explain the meaning of the CI in the R-output for this test

##### Propose and perform a suitable sign tests for this problem.
To test about a sample medianm, we can also use a sign test. We propose a sign test where the H0 is median = 2800 and the H1 is median > 2800. we'll call 2800 the m0. from this follows the test: T = #(x>2800). Following the arguments stated in 1a, this is a one-sided test.
```{r}
nT = length(x[x>2800]); 
pttest = t.test(x,mu=2800,alt="g")[[3]]
psign = binom.test(nT,n,p=0.5)[[3]]
```

Now we can compare the results from both tests
```{r}
pttest
psign
```

When looking at the resulting p values from both tests, we can see that the null hypothesis doesn't get rejected by the sign test, but that the t-test does succeed in providing significant results.  

@GIJS: which is better?

### C
Compute powers -> look at this

Wordt nog gefixt?

#### Propose a way to compute the powers of the t-test and sing test from b) at some mu>2800, comment.

### D
#### Let p be the probability that birthweight of a newborn baby is less than 2600 gram. Using asymptotic normality, the expert computed the left end p-hat-l = 0.25 of the confidence interval [p-hat-l, p-hat-r] for p. Recover the whole confidence interval and its confidence level.

Ik ben een beetje lost bij deze

```{r}
q = length(x[x<2600])/n - 0.25
pr = length(x[x<2600])/n + q
```

```{r}
ci_p = c(0.25,pr)
```
Confidence interval is [`r round(ci_p,3)`]

Now compute confidence level, by (math): q = z_(a/2) * sd/sqrt(n), so z_(a/2) = q * sqrt(n) / sd
```{r}
z = q * (sqrt(n) / sd)
pnorm(z)
```

Is this correct? -> alpha. Look at this

### E
#### The expert also reports that there were 34 male and 28 female babies among 62 who weighted less than 2600 gram, and 61 male and 65 female babies among the remaining 126 babies. The expert claims that the mean weight is different for male and female babies. Verify this claim by an appropriate test.

To verify this claim we have to test for a difference in proportions. This can be done with `prop.test()` in R. This test tests whether the proportion of 'success' between two population differs. In order to use this test, we have to assume normality of the data. In this case we take 'succes' to mean weighing less than 2600 grams. This way we can test whether the proportion of babies weighing less than 2600 grams is different between males and females.

```{r}
# <2600g: 34 males, 28 females
# >2600g: 61 males, 65 females.
male = 34 + 61
female = 28 + 65

prop.test(c(34,28),c(male,female))
```
The results from this test show us that there is no significant difference between the proportion of babies weighing less than 2600 grams between males and females.

## Exercise 2

### A
```{r}
data = read.table('../data/cholesterol.txt',header=TRUE); data
before = data$Before
after = data$After8weeks
summary(data)
shapiro.test(before)
shapiro.test(after)
```
```{r,echo=FALSE}
reg = lm(after ~ before)
plot(before,after, pch=16, col="blue")
abline(reg)
```
summary(reg) adj R^2 = 0.98 explained, correlated positively, but less than 1 so cholesterol level gets less. 

But easier to see, if we remove before bias and only look at first differences:
```{r,echo=FALSE}
diff = after - before
reg2 = lm(diff ~ before)
plot(before,diff, pch=16, col="red")
abline(reg2)
summary(reg2)
```
Notice that decrease in cholesterol level is higher for people who have higher cholesterol level before the experiment

### B

### C

### D

### E


## Exercise 3

### A

### B

### C

### D

### E


## Exercise 4

### A

### B

### C

### D

### E

