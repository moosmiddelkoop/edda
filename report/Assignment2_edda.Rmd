---
title: "Assignment 2"
author: "Gijs Smeets, Daan Wijnhorst, Moos Middelkoop group 35"
date: "February 2023"
output:
  html_document:
    df_print: paged
fontsize: 11pt
highlight: tango
---

## Exercise 2

### A


```{r}
data = read.table('../data/cholesterol.txt',header=TRUE)
before = data$Before
after = data$After8weeks
```
To check for normality, we look at the Q-Q plots
```{r,echo=FALSE, fig.margin = FALSE,fig.width=12,fig.height=6,fig.align="center"}
par(mfrow=c(1,2))
qqnorm(before,main="Q-Q Plot Before")
qqnorm(after,main="Q-Q Plot After")
```
Here we see that the sampled data is approximately normal
```{r}
summary(data)
shapiro.test(before)
shapiro.test(after)
```
To complement the visual check, we use the Shapiro-Wilk test. 
In this test, the H0 is that the data is normal. H1 is that the data is not normal.

As the p-value from the Shapiro-Wilk test is high for both values we do not have enough evidence to reject the null-hypothesis.
If we combine this result with the visual check we can safely assume the data to be normally distributed.

```{r,echo=FALSE, fig.margin = FALSE,fig.width=8,fig.height=6,fig.align="center"}
par(mfrow = c(2,2))
hist(before); hist(after)
boxplot(before); boxplot(after)
```


```{r,echo=FALSE, fig.margin = FALSE,fig.width=12,fig.height=6,fig.align="center"}
par(mfrow=c(1,2))
reg = lm(after ~ before)
plot(before,after, pch=16, col="blue")
abline(reg)
summary(reg) 
diff = after - before
reg2 = lm(diff ~ before)
plot(before,diff, pch=16, col="red")
abline(reg2)
summary(reg2)
```
adj R^2 = 0.98 explained, correlated positively.
From the plots it becomes clear that people who have higher cholesterol levels in the before data have a higher absolute decrease of cholesterol after the experiment.
It becomes easier to see if we remove before bias and only look at the differences (see the right red plot)

### B
#### Pearson Correlation test
As we have concluded the data to be normally distributed in A, we can conduct a Paired-Sampled T-test.
In this test, H0: the mean difference between the values of X and Y are 0 H1: this is not the case, the difference is not 0.

```{r} 
#T-test
t.test(before, after, paired = TRUE)
```
As the p-value is below 0.05 we reject H0 of the T-test and conclude the mean paired differences are not equal to zero.

Another relevant test is testing for correlation.

We use the Pearson's correlation test as we have considered the data to be normally distributed in subsection A.

```{r, fig.margin = FALSE,fig.width=8,fig.height=4,fig.align="center"} 
#pearson corr. test
par(mfrow=c(1,2))
cor.test(before,after)
```
Pearson's returns the correlation value of 0.9908885 herefore we conclude there exists a strong (positive) correlation between the two variables.

Combining the results of the two tests, we conclude that the low fat margarine diet has an effect.

#### Is a permutation test applicable?
As we are dealing with numerical outcomes, two conditions per experimental unit and we are interested in possible differences between two outcomes per unit, we can devise a permutation test.

```{r, fig.margin = FALSE,fig.width=6,fig.height=4,fig.align="center"} 
meansamples = function(x,y) {mean(x-y)}
B=1000; Tstar = numeric(B)
for (i in 1:B){
  dietstar = t(apply(cbind(before,after),1,sample))
  Tstar[i] = meansamples(dietstar[,1],dietstar[,2])
}
myt = meansamples(before,after);
hist(Tstar)
```
```{r} 
pl = sum(Tstar<myt)/B
pr = sum(Tstar>myt)/B
p = 2*min(pl,pr)
p
```
As P = 0 we conclude that there is indeed a significant difference between the before and after data.

### C
As we can assume from the assignment that the sample has an underlying uniform distribution, we can construct the point estimate as followed:
```{r} 
# function that computes variance of uniformly distr. variable
unif_var = function(a,b){
  return ((1/12)*((b-a)^2))
}

# E(X) = (a+b)/2 = mean(after)
# So point estimate of b is 2*E(X)-3
theta_est = 2*mean(after)-3
```
Now we have the point estimate, we can construct the confidence interval for level 1-alpha. As we know sigma we can compute the CI as follows:
```{r}
#build 95%-CI for theta_est
n = length(after)
ci_theta = c(theta_est - qnorm(0.975)*(sqrt(unif_var(3,theta_est))/sqrt(n)), theta_est + qnorm(0.975)*(sqrt(unif_var(3,theta_est))/sqrt(n)))
ci_theta


```
This returns us the interval 7.816600, 9.298956.
we can try to improve the CI via choosing a different estimating statistic from the population (the median for example) and look at the difference between the returned interval and the new interval this looks as follows:


### D

#bootstrap test
```{r} 
t = max(after); t

vP = rep(0,9)
vT = rep(0,9)
for (theta in 3:12){
  B = 1000; tstar=numeric(B)
  
  for (i in 1:B){
    xstar=runif(n,3,theta)
    tstar[i] = max(xstar)
  }
  
  pl = sum(tstar<t)/B; pr = sum(tstar>t)/B
  p = 2*min(pl,pr); p
  
  vT[theta-2] = theta
  vP[theta-2] = p
  
}
res = rbind(vT,vP); res

```
Kolmogorov-Smirnov test can be applied as this test tests for H0 that the two undferlying distributions are the same. The uniform distribution can be distribution 1 and the sampled data can be distribution 2.

### E
```{r} 
## EXERCISE 2E
# sign test for the median with binomial distr.
s = sum(after<6); s
binom.test(s,n,p=0.5,alt="g") 

# fraction of cholesterol levels lower than 4.5 in after is at most 0.25
s2 = (sum(after<4.5))
binom.test(s2,n,p=0.25,alt="l")
```